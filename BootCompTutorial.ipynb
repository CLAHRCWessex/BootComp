{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap multiple comparisons tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide to using BootComp to perform multiple comparisons of simulation output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with management and house keeping BootComp is split up over a number of Python modules.  We need to import these into the notebook before we can run any analysis.\n",
    "\n",
    "1. Bootstrap = core bootstrap routines for resampling and calculating test statistics. \n",
    "2. BootIO = file input and output functions as well as functions for displaying results on screen\n",
    "3. BootChartExtensions - MatPlotLib functions for displaying comparions using charts\n",
    "4. MCC - multiple comparison utility functions e.g. Bonferroni adjustment\n",
    "5. ConvFuncs - utility module contains functions for converting between datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Bootstrap as bs\n",
    "import BootIO as io\n",
    "#import BootChartExtensions as ch\n",
    "import MCC as mcc\n",
    "import ConvFuncs as cf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to make use of a Python Data Science Library called PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_BOOTS = 2000\n",
    "INPUT_DATA = \"data/real_scenarios.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenarios need to be in a .csv file.  Each scenario needs to have the same number of replications/batch means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#note BootStrap routines require lists of lists\n",
    "scenario_data = cf.list_of_lists(io.load_scenarios(INPUT_DATA))\n",
    "N_SCENARIOS = len(scenario_data)\n",
    "print(\"Loaded data. {0} scenarios\".format(N_SCENARIOS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the bootstrap we need to setup a BootstrapArguments class.  The arguments class is a a container class that encapsulates all arguments for the bootstrap.  Some of these arguments are simple value based ones e.g. the number of bootstrap resamples and others are complex functions e.g. the test statisic we will calculate on each bootstrap resample.  \n",
    "\n",
    "The ones we are going to use in this example are:\n",
    "\n",
    "1. nboot = the number of bootstrap resamples to run.\n",
    "2. nscenarios = the number of scenarios to compare\n",
    "3. ncomparisons = the number of simulataneious comparisons \n",
    "4. confidence = the the confidence level we will use within any 100%(1-alpha) Confidence interval we contruct\n",
    "5. point_estimate_func = A function that takes a set of bootstrap samples as parameter and calculates a point estimate.        e.g the mean or standard deviation.  signature = func(data, args).\n",
    "6. test_statistic_function = a function that calculates a test statistic for comparing two scenarios (e.g.             difference). method signature is func(first_scenario_data, second_scenario_data)\n",
    "7. comp_function = how do you want to analyse the test statistic?  Percentile CIs? Proportion Sj > Si?  Or graphically?  Specifiy a function of signature func(data, args).\n",
    "\n",
    "For this example we will use a test statistic of the mean difference and the comparison will be the proportion of Sj > Si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args =  bs.BootstrapArguments()\n",
    "\n",
    "args.nboots = N_BOOTS\n",
    "args.nscenarios = N_SCENARIOS\n",
    "\n",
    "args.point_estimate_func = bs.bootstrap_mean\n",
    "args.test_statistic_function = bs.boot_mean_diff\n",
    "args.comp_function = bs.proportion_x2_lessthan_x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a dataset containing resampled test statistics for each scenario you need to run the resample_all_scenarios function from the Bootstrap module.  It takes two parameters i.) the scenario data ii.) A BootstrapArguments object.\n",
    "\n",
    "The first example performs 10 bootstrap resamples of the mean for illustration of the data returned.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_data = bs.resample_all_scenarios(scenario_data, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boot_data is a list of lists.  Each lists contains the resampled point estimates from the simulation experiment/scenario.  To visualise this data we are going to use a PANDAS DataFrame object.  When displayed each column represents a experiment/scenario and each row represents a bootstrap sample.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boot_data, columns = [str(i) for i in range(1, args.nboots+1)])\n",
    "df.index += 1\n",
    "df2 = df.transpose()\n",
    "df2.to_csv(\"resamples.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Comparing scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the compare_scenarios_pairwise function to conduct an all pairwise comparison of the bootstrapped scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = bs.compare_scenarios_pairwise(boot_data, args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Printing the results of a comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two options for printing results to the screen.  For a large number of comparisons it is receommended that you use 'long format'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io.print_long_format_comparison_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a small number of comparisons it is recommended that you use matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = io.results_to_matrix(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include the missing cells in the comparison table above use the insert_inverse_results function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io.insert_inverse_results(matrix, args.nscenarios)\n",
    "df = io.matrix_to_dataframe(matrix, io.scenario_headers(args.nscenarios))\n",
    "df.style.applymap(io.colour_cells_by_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Writing results to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the results are in matrix format you can write these to file with a single function.  The file is written to the working directory and is called results_matrix.csv. The results can be opened in Excel (or otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io.write_results_matrix(matrix, io.scenario_headers(args.nscenarios))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visual comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If required a visual comparison of scenarios could be conducted. Chart based comparison functions are contained in the BootChartExtensions module.  Here we illustrate plotting the PDF of mean differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#args.comp_function = ch.plot_boostrap_samples_pdf\n",
    "#bs.compare_scenarios_pairwise(boot_data, args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we illustrate plotting the CDF of mean differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#args.comp_function = ch.plot_boostrap_samples_cdf\n",
    "#bs.compare_scenarios_pairwise(boot_data, args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Ranking Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 Best systems - proportion of resamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help identify the top systems found in experimentation use Bootstrap.rank_systems_min and Bootstrap.rank_systems_max.  These return a frequency table reporting the number of proportion of bootstrap resamples where system i was the best (minimum or maximium).  Results exclude systems that never came top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(bs.rank_systems_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_boots = cf.resamples_to_df(boot_data, args.nboots)  #put the bootstrap data into a Pandas.DataFrame\n",
    "bs.rank_systems_min(df_boots, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 Proportion of time system is the best m systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can find the proportion of resamples where a system was in the best m systems.\n",
    "I.e. What were the proportion of 1000 bootstrap resamples where the results for system 1 was in the smallest 5 of all of the systems.  \n",
    "\n",
    "There are functions:\n",
    "1. Bootstrap.rank_systems_msmallest(DataFrame, BootstrapArguments, m)\n",
    "2. Bootstrap.rank_systems_mlargest(DataFrame, BootstrapArguments, m)\n",
    "\n",
    "Note that when m = 1 these functions are equivalent to those introduced in section 2.6.1\n",
    "\n",
    "For this example the output measure related to waiting times.  We therefore use function 1 above converned with finding systems that consistently rank amoung the lowest 5 waiting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(bs.rank_systems_msmallest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msmallest = bs.rank_systems_msmallest(df_boots, args, 5)\n",
    "msmallest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 Rerunning comparisons using 'the best' subset of all systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking analysis from 2.6.1. provides a shortlist of the systems that consistently appear in the best m systems.  This information can be used to perform a comparison of the subset of systems with the most promicing outputs.  First we need to do two things:\n",
    "1.) Get the list of system/scenario ids as a list\n",
    "2.) Get the subset of bootstrap resamples corresponding to this subset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_indexes = msmallest.index.values.tolist()\n",
    "subset = cf.subset_of_list(boot_data, subset_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we update the BootStrapArguments obeject: i.e. the number of scenarios we are working with and the type of comparison we want to do. Then run the compare_scenarios_pairwise function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args.nscenarios = len(subset)\n",
    "args.comp_function = bs.proportion_x2_lessthan_x1\n",
    "results = bs.compare_scenarios_pairwise(subset, args) \n",
    "matrix = io.results_to_matrix(results) \n",
    "io.insert_inverse_results(matrix, args.nscenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = io.matrix_to_dataframe(matrix, [str(i) for i in subset_indexes])\n",
    "df.style.applymap(io.colour_cells_by_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To code below limits the comparison to the top 10 ranked systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_indexes = msmallest.index.values.tolist()[:10]\n",
    "subset = cf.subset_of_list(boot_data, subset_indexes)\n",
    "args.nscenarios = len(subset)\n",
    "args.comp_function = bs.proportion_x2_lessthan_x1\n",
    "results = bs.compare_scenarios_pairwise(subset, args) \n",
    "\n",
    "matrix = io.results_to_matrix(results) \n",
    "io.insert_inverse_results(matrix, args.nscenarios)\n",
    "\n",
    "df = io.matrix_to_dataframe(matrix, [str(i) for i in subset_indexes])\n",
    "\n",
    "df.style.applymap(io.colour_cells_by_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "subset = cf.subset_of_list(scenario_data, subset_indexes)\n",
    "[mean(i) for i in subset]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

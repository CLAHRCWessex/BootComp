{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap multiple comparisons tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide to using BootComp to perform multiple comparisons of simulation output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with management and house keeping BootComp is split up over a number of Python modules.  We need to import these into the notebook before we can run any analysis.\n",
    "\n",
    "1. Bootstrap = core bootstrap routines for resampling and calculating test statistics. \n",
    "2. BootIO = file input and output functions as well as functions for displaying results on screen\n",
    "3. BootChartExtensions - MatPlotLib functions for displaying comparions using charts\n",
    "4. MCC - multiple comparison utility functions e.g. Bonferroni adjustment\n",
    "5. ConvFuncs - utility module contains functions for converting between datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Bootstrap as bs\n",
    "import BootIO as io\n",
    "import BootChartExtensions as ch\n",
    "import MCC as mcc\n",
    "import ConvFuncs as cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to make use of a Python Data Science Library called PANDAS and two plotting librairies called Matplotlib and Seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_BOOTS = 2000\n",
    "INPUT_DATA = \"data/real_scenarios.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenarios need to be in a .csv file.  Each scenario needs to have the same number of replications/batch means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scenario_data = io.load_scenarios(INPUT_DATA)\n",
    "N_SCENARIOS = len(scenario_data)\n",
    "print(\"Loaded data. {0} scenarios\".format(N_SCENARIOS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the bootstrap we need to setup a BootstrapArguments class.  The arguments class is a a container class that encapsulates all arguments for the bootstrap.  Some of these arguments are simple value based ones e.g. the number of bootstrap resamples and others are complex functions e.g. the test statisic we will calculate on each bootstrap resample.  \n",
    "\n",
    "The ones we are going to use in this example are:\n",
    "\n",
    "1. nboot = the number of bootstrap resamples to run.\n",
    "2. nscenarios = the number of scenarios to compare\n",
    "3. ncomparisons = the number of simulataneious comparisons \n",
    "4. confidence = the the confidence level we will use within any 100%(1-alpha) Confidence interval we contruct\n",
    "5. point_estimate_func = A function that takes a set of bootstrap samples as parameter and calculates a point estimate.        e.g the mean or standard deviation.  signature = func(data, args).\n",
    "6. difference_func = a function that calculates the difference between two scenarios (e.g a mean difference). method signature is func(first_scenario_data, second_scenario_data)\n",
    "7. summary_func = how do you want to summarise the comparisons?  Percentile CIs? Proportion Sj > Si?  Or graphically?  Specifiy a function of signature func(data, args).\n",
    "\n",
    "For this example we will use a test statistic of the mean difference and the comparison will be the proportion of Sj > Si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args =  bs.BootstrapArguments()\n",
    "\n",
    "args.nboots = N_BOOTS\n",
    "args.nscenarios = N_SCENARIOS\n",
    "\n",
    "args.point_estimate_func = bs.bootstrap_mean\n",
    "args.difference_func = bs.boot_mean_diff\n",
    "args.summary_func = bs.proportion_x2_lessthan_x1\n",
    "#args.summary_func = bs.proportion_x2_greaterthan_x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a dataset containing resampled test statistics for each scenario you need to run the resample_all_scenarios function from the Bootstrap module.  It takes two parameters i.) the scenario data ii.) A BootstrapArguments object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boot_data = bs.resample_all_scenarios(scenario_data, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boot_data is a list of lists.  Each lists contains the resampled point estimates from the simulation experiment/scenario.  To visualise this data we are going to use a PANDAS DataFrame object.  When displayed each column represents a experiment/scenario and each row represents a bootstrap sample.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(cf.resamples_to_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_boots = cf.resamples_to_df(boot_data, N_BOOTS)\n",
    "df_boots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_boots.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Comparing scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the compare_scenarios_pairwise function to conduct an all pairwise comparison of the bootstrapped scenarios.  Note this assumes independence across scenarios/systems modelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = bs.compare_scenarios_pairwise(boot_data, args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Printing the results of a comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two options for printing results to the screen.  'Long' and 'Matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io.print_long_format_comparison_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix format compares the scenarios by row and column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = io.results_to_matrix(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the matrix is diagnoally only half complete (as the 2nd half is is the inverse of the first). To include the missing cells in the comparison table above use the insert_inverse_results function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io.insert_inverse_results(matrix, args.nscenarios)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the results it is again handy to convert the matrix to a dataframe.  You can then apply a map to colour code the cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = cf.matrix_to_dataframe(matrix, io.scenario_headers(args.nscenarios))\n",
    "df.style.applymap(io.colour_cells_by_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Writing results to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the results are in matrix format you can write these to file with a single function.  The file is written to the working directory and is called results_matrix.csv. The results can be opened in Excel (or otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io.write_results_matrix(matrix, io.scenario_headers(args.nscenarios))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Ranking Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.1 Rank by Wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help identify the top systems found in experimentation use Bootstrap.rank_systems_min and Bootstrap.rank_systems_max.  These return a frequency table reporting the number of proportion of bootstrap resamples where system i was the best (minimum or maximium).  Results exclude systems that never came top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(bs.rank_systems_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranks = bs.rank_systems_min(df_boots, args)\n",
    "s_ranks = ranks.sort_values(\"f_x\", ascending=False)\n",
    "s_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_boots.to_csv('resamples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax= s_ranks.plot(x=ranks.index, y='f_x', kind='bar')\n",
    "ax.set_ylabel('frequency (best)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(\"rank_1.tif\", format = 'tif', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 Top m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can find the proportion of resamples where a system was in the best m systems.\n",
    "I.e. What were the proportion of 1000 bootstrap resamples where the results for system 1 was in the smallest 5 of all of the systems.  \n",
    "\n",
    "There are two two functions:\n",
    "1. Bootstrap.rank_systems_msmallest(DataFrame, BootstrapArguments, m)\n",
    "2. Bootstrap.rank_systems_mlargest(DataFrame, BootstrapArguments, m)\n",
    "\n",
    "Note that when m = 1 these functions are equivalent to those introduced in section 2.6.1\n",
    "\n",
    "For this example the output measure related to waiting times.  We therefore use function 1 above converned with finding systems that consistently rank amoung the lowest 5 waiting times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(bs.rank_systems_msmallest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msmallest = bs.rank_systems_msmallest(df_boots, args, 5)\n",
    "msmallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_ranks = msmallest.sort_values(\"f_x\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax= s_ranks.plot(x=s_ranks.index, y='f_x', kind='bar')\n",
    "ax.set_ylabel('frequency (within best 5 systems)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(\"rank_2.tif\", format = 'tif', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 Rerunning comparisons using 'the best' subset of all systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking analysis from 2.6.1. provides a shortlist of the systems that consistently appear in the best m systems.  This information can be used to perform a comparison of the subset of systems with the most promising outputs.  First we need to do two things:\n",
    "1.) Get the list of system/scenario ids as a list\n",
    "2.) Get the subset of bootstrap resamples corresponding to this subset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_indexes = msmallest.index.values.tolist()\n",
    "print(subset_indexes)\n",
    "#zero indexed\n",
    "subset_indexes_zero = [x - 1 for x in subset_indexes]\n",
    "\n",
    "subset = cf.subset_of_list(boot_data, subset_indexes_zero)\n",
    "print(subset[0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_boots[40].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we update the BootStrapArguments object: i.e. the number of scenarios we are working with and the type of comparison we want to do. Then run the compare_scenarios_pairwise function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args.nscenarios = len(subset)\n",
    "args.summary_func = bs.proportion_x2_lessthan_x1\n",
    "#args.summary_func = bs.proportion_x2_greaterthan_x1\n",
    "results = bs.compare_scenarios_pairwise(subset, args) \n",
    "matrix = io.results_to_matrix(results) \n",
    "io.insert_inverse_results(matrix, args.nscenarios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = cf.matrix_to_dataframe(matrix, [str(i) for i in subset_indexes])\n",
    "df.style.applymap(io.colour_cells_by_proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To code below limits the comparison to the top 13 ranked systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_indexes = msmallest.index.values.tolist()[:13]\n",
    "subset_indexes_zero = [x - 1 for x in subset_indexes]\n",
    "subset = cf.subset_of_list(boot_data, subset_indexes_zero)\n",
    "args.nscenarios = len(subset)\n",
    "args.summary_func = bs.proportion_x2_lessthan_x1\n",
    "results = bs.compare_scenarios_pairwise(subset, args) \n",
    "\n",
    "matrix = io.results_to_matrix(results) \n",
    "io.insert_inverse_results(matrix, args.nscenarios)\n",
    "\n",
    "df = cf.matrix_to_dataframe(matrix, [str(i) for i in subset_indexes])\n",
    "\n",
    "df.style.applymap(io.colour_cells_by_proportion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.3 Visual Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "subset_indexes = msmallest.index.values.tolist()[:13]\n",
    "ax = df_boots[subset_indexes].plot.box()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "subset = cf.subset_of_list(scenario_data, subset_indexes)\n",
    "print([mean(i) for i in subset])\n",
    "print(subset_indexes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

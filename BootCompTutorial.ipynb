{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap multiple comparisons tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a step-by-step guide to using BootComp to perform multiple comparisons of simulation output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help with management and house keeping BootComp is split up over a number of Python modules.  We need to import these into the notebook before we can run any analysis.\n",
    "\n",
    "1. Bootstrap = core bootstrap routines for resampling and calculating test statistics. \n",
    "2. BootIO = file input and output functions as well as functions for displaying results on screen\n",
    "3. BootChartExtensions - MatPlotLib functions for displaying comparions using charts\n",
    "4. MCC - multiple comparison utility functions e.g. Bonferroni adjustment\n",
    "5. ConvFuncs - utility module contains functions for converting between datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Bootstrap as bs\n",
    "import BootIO as io\n",
    "import BootChartExtensions as ch\n",
    "import MCC as mcc\n",
    "import ConvFuncs as cf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to make use of a Python Data Science Library called PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONFIDENCE = 95\n",
    "N_BOOTS = 1000\n",
    "INPUT_DATA = \"data/mini_scenarios.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenarios need to be in a .csv file.  Each scenario needs to have the same number of replications/batch means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#note BootStrap routines require lists of lists\n",
    "scenario_data = cf.list_of_lists(io.load_scenarios(INPUT_DATA))\n",
    "N_SCENARIOS = len(scenario_data)\n",
    "print(\"Loaded data. {0} scenarios\".format(N_SCENARIOS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the bootstrap we need to setup a BootstrapArguments class.  The arguments class is a a container class that encapsulates all arguments for the bootstrap.  Some of these arguments are simple value based ones e.g. the number of bootstrap resamples and others are complex functions e.g. the test statisic we will calculate on each bootstrap resample.  \n",
    "\n",
    "The ones we are going to use in this example are:\n",
    "\n",
    "1. nboot = the number of bootstrap resamples to run.\n",
    "2. nscenarios = the number of scenarios to compare\n",
    "3. ncomparisons = the number of simulataneious comparisons \n",
    "4. confidence = the the confidence level we will use within any 100%(1-alpha) Confidence interval we contruct\n",
    "5. point_estimate_func = A function that takes a set of bootstrap samples as parameter and calculates a point estimate.        e.g the mean or standard deviation.  signature = func(data, args).\n",
    "6. test_statistic_function = a function that calculates a test statistic for comparing two scenarios (e.g.             difference). method signature is func(first_scenario_data, second_scenario_data)\n",
    "7. comp_function = how do you want to analyse the test statistic?  Percentile CIs? Proportion Sj > Si?  Or graphically?  Specifiy a function of signature func(data, args).\n",
    "\n",
    "For this example we will use a test statistic of the mean difference and the comparison will be the proportion of Sj > Si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args =  bs.BootstrapArguments()\n",
    "\n",
    "args.nboots = N_BOOTS\n",
    "args.nscenarios = N_SCENARIOS\n",
    "\n",
    "args.point_estimate_func = bs.bootstrap_mean\n",
    "args.test_statistic_function = bs.boot_mean_diff\n",
    "args.comp_function = bs.proportion_x2_greaterthan_x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a dataset containing resampled test statistics for each scenario you need to run the resample_all_scenarios function from the Bootstrap module.  It takes two parameters i.) the scenario data ii.) A BootstrapArguments object.\n",
    "\n",
    "The first example performs 10 bootstrap resamples of the mean for illustration of the data returned.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args.nboots = 10\n",
    "boot_data = bs.resample_all_scenarios(scenario_data, args)\n",
    "print(\"Resampling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boot_data is a list of lists.  Each lists contains the resampled point estimates from the simulation experiment/scenario.  To visualise this data we are going to use a PANDAS DataFrame object.  When displayed each column represents a experiment/scenario and each row represents a bootstrap sample.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(boot_data, columns = [str(i) for i in range(1, args.nboots+1)])\n",
    "df.index += 1\n",
    "df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second example performs 1000 bootstrap resamples and will be used for analysis.  We also output the time taken to complete the resampling analysis using the IPython magic %timeit (remove this in a normal or large analysis as it runs the code multiple times). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args.nboots = N_BOOTS\n",
    "%timeit boot_data = bs.resample_all_scenarios(scenario_data, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Comparing scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the compare_scenarios_pairwise function to conduct an all pairwise comparison of the bootstrapped scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = bs.compare_scenarios_pairwise(boot_data, args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Printing the results of a comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two options for printing results to the screen.  For a large number of comparisons it is receommended that you use 'long format'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io.print_long_format_comparison_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a small number of comparisons it is recommended that you use matrix format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = io.results_to_matrix(results) \n",
    "io.print_results_matrix(matrix, N_SCENARIOS) #To DO. only works if proportion comparison performed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To include the missing cells in the comparison table above use the insert_inverse_results function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "io.insert_inverse_results(matrix, N_SCENARIOS)\n",
    "io.print_results_matrix(matrix, N_SCENARIOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Writing results to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the results are in matrix format you can write these to file with a single function.  The file is written to the working directory and is called results_matrix.csv. The results can be opened in Excel (or otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.write_results_matrix(matrix, N_SCENARIOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visual comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If required a visual comparison of scenarios could be conducted. Chart based comparison functions are contained in the BootChartExtensions module.  Here we illustrate plotting the PDF of mean differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args.comp_function = ch.plot_boostrap_samples_pdf\n",
    "bs.compare_scenarios_pairwise(boot_data, args) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we illustrate plotting the CDF of mean differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "args.comp_function = ch.plot_boostrap_samples_cdf\n",
    "bs.compare_scenarios_pairwise(boot_data, args) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
